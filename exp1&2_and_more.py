# -*- coding: utf-8 -*-
"""EXP1&2 and more

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15fDJPZruCorctu41KjudltnTmWDt_Yhj

Standard Analysis
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive',force_remount=True)
file_path = '/content/sample_data/mnist_train_small.csv'
df = pd.read_csv(file_path)

print(df.head())
print(df.info())
print(df.describe())
print(df.columns)
print(df.dtypes)
mean_values = df.mean()


median_values = df.median()


mode_values = df.mode().iloc[0]


std_dev_values = df.std()


print("Mean values:\n", mean_values)
print("\nMedian values:\n", median_values)
print("\nMode values:\n", mode_values)
print("\nStandard Deviation values:\n", std_dev_values)

"""Bayesian Logistic Regression


"""

import torch
import torch.nn as nn
import pyro
import pyro.distributions as dist
from pyro.infer import SVI, Trace_ELBO
from pyro.optim import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
import numpy as np

# Step 1: Load data
file_path = '/content/sample_data/mnist_train_small.csv'
df = pd.read_csv(file_path)

# Step 2: Separate features and labels
X = df.iloc[:, 1:].values  # Features (pixels)
y = df.iloc[:, 0].values   # Labels (digits)

# For simplicity, let's focus on binary classification (e.g., 0 and 1)
binary_filter = np.isin(y, [0, 1])
X_binary = X[binary_filter]
y_binary = y[binary_filter]

# Step 3: Normalize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_binary)

# Convert to PyTorch tensors
X_tensor = torch.tensor(X_scaled, dtype=torch.float)
y_tensor = torch.tensor(y_binary, dtype=torch.float)

# Step 4: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)

# Step 5: Define the Bayesian Logistic Regression model in Pyro
def model(X, y=None):
    # Priors for weights and intercept
    weights = pyro.sample('weights', dist.Normal(torch.zeros(X.shape[1]), torch.ones(X.shape[1])).to_event(1))
    intercept = pyro.sample('intercept', dist.Normal(torch.tensor(0.), torch.tensor(10.)))

    # Linear combination (logits)
    logits = torch.matmul(X, weights) + intercept

    # Likelihood (Bernoulli likelihood with logits for binary classification)
    with pyro.plate('data', X.shape[0]):
        pyro.sample('obs', dist.Bernoulli(logits=logits), obs=y)

# Step 6: Define the guide (variational distribution for inference)
def guide(X, y=None):
    # Variational parameters for weights
    weights_loc = pyro.param('weights_loc', torch.zeros(X.shape[1]))
    weights_scale = pyro.param('weights_scale', torch.ones(X.shape[1]), constraint=dist.constraints.positive)

    # Variational parameters for intercept
    intercept_loc = pyro.param('intercept_loc', torch.tensor(0.))
    intercept_scale = pyro.param('intercept_scale', torch.tensor(1.), constraint=dist.constraints.positive)

    # Sample from the variational distribution (approximate posterior)
    pyro.sample('weights', dist.Normal(weights_loc, weights_scale).to_event(1))
    pyro.sample('intercept', dist.Normal(intercept_loc, intercept_scale))

# Step 7: Set up the optimizer and inference method (Stochastic Variational Inference)
adam = Adam({"lr": 0.01})
svi = SVI(model, guide, adam, loss=Trace_ELBO())

# Step 8: Train the model (run inference)
num_steps = 5000  # Number of steps to run the optimizer
for step in range(num_steps):
    loss = svi.step(X_train, y_train)
    if step % 500 == 0:
        print(f"Step {step} : loss = {loss}")

# Step 9: Get the learned parameters from the variational distribution
weights_loc = pyro.param('weights_loc').detach().numpy()
intercept_loc = pyro.param('intercept_loc').item()

# Step 10: Make predictions on the test set
with torch.no_grad():
    logits_test = torch.matmul(X_test, torch.tensor(weights_loc)) + intercept_loc
    y_pred_prob = torch.sigmoid(logits_test).numpy()

# Step 11: Convert probabilities to binary predictions
y_pred = (y_pred_prob > 0.5).astype(int)

# Step 12: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

# Detailed classification report
print(classification_report(y_test, y_pred))

"""Linear Regression"""

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
import numpy as np

# Step 1: Load the data (MNIST dataset as an example)
file_path = '/content/sample_data/mnist_train_small.csv'
df = pd.read_csv(file_path)

# Use one column (for simplicity, we'll predict the first pixel value in the image) as the target.
# Normally in linear regression, you'd use real continuous data, but for illustration, let's predict pixel[0].
X = df.iloc[:, 1:].values  # Features (pixels)
y = df.iloc[:, 1].values   # Let's predict the value of the first pixel

# Step 2: Normalize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert to PyTorch tensors
X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # Add an extra dimension for target

# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)

# Step 4: Define a simple Linear Regression model using PyTorch
class LinearRegressionModel(nn.Module):
    def __init__(self, input_dim):
        super(LinearRegressionModel, self).__init__()
        # Single linear layer: input_dim -> 1 (for one output, which is the predicted value)
        self.linear = nn.Linear(input_dim, 1)

    def forward(self, x):
        return self.linear(x)

# Initialize the model
input_dim = X_train.shape[1]  # Number of input features (pixels)
model = LinearRegressionModel(input_dim)

# Step 5: Define the loss function (Mean Squared Error) and the optimizer (Stochastic Gradient Descent)
criterion = nn.MSELoss()  # Mean Squared Error Loss
optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent

# Step 6: Train the model
num_epochs = 1000  # Number of training epochs
for epoch in range(num_epochs):
    model.train()  # Set model to training mode

    # Forward pass: Compute predicted y by passing X_train to the model
    y_pred = model(X_train)

    # Compute the loss
    loss = criterion(y_pred, y_train)

    # Backward pass: Compute gradients
    optimizer.zero_grad()  # Zero the gradients before running the backward pass
    loss.backward()        # Backpropagate the loss

    # Update the weights
    optimizer.step()       # Perform one optimization step

    if (epoch+1) % 100 == 0:
        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')

# Step 7: Evaluate the model
model.eval()  # Set model to evaluation mode

# Make predictions on the test set
with torch.no_grad():
    y_pred_test = model(X_test)
    y_pred_test = y_pred_test.detach().numpy()

# Step 8: Calculate evaluation metrics
y_test_np = y_test.numpy()
mse = mean_squared_error(y_test_np, y_pred_test)
r2 = r2_score(y_test_np, y_pred_test)

print(f'Mean Squared Error: {mse:.4f}')
print(f'R-squared: {r2:.4f}')

"""SVM Classifier"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Step 1: Load data
file_path = '/content/sample_data/mnist_train_small.csv'
df = pd.read_csv(file_path)

# Step 2: Separate features and labels
X = df.iloc[:, 1:].values  # Features (pixels)
y = df.iloc[:, 0].values   # Labels (digits)

# Step 3: Normalize the data (optional but recommended for SVM)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 4: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 5: Train the SVM classifier
svm_model = SVC(kernel='rbf')  # You can try 'rbf', 'poly' for different kernels
svm_model.fit(X_train, y_train)

# Step 6: Make predictions
y_pred = svm_model.predict(X_test)

# Step 7: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

# Detailed classification report
print(classification_report(y_test, y_pred))

"""Principal Componenent Analysis(PCA) Of MNIST DATA"""

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load the dataset
file_path = '/content/sample_data/mnist_train_small.csv'
df = pd.read_csv(file_path)

# Separate the features (assuming the first column is labels)
X = df.iloc[:, 1:].values  # features
y = df.iloc[:, 0].values   # labels (if needed)

# Standardize the data (PCA is sensitive to the scale of the data)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform PCA
pca = PCA(n_components=2)  # Change n_components to reduce dimensions as needed
X_pca = pca.fit_transform(X_scaled)

# Explained variance ratio
explained_variance = pca.explained_variance_ratio_
print(f"Explained Variance Ratio: {explained_variance}")

# Plot the PCA result
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=5)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of MNIST Data')
plt.colorbar(label='Label')
plt.show()

"""K-Means Clustering of MNIST Data

"""

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load the dataset
file_path = '/content/sample_data/mnist_train_small.csv'
df = pd.read_csv(file_path)

# Separate the features (assuming the first column is labels)
X = df.iloc[:, 1:].values  # features (pixels)
y = df.iloc[:, 0].values   # labels (if needed)

# Standardize the data (K-Means is sensitive to the scale of the data)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform K-Means clustering
kmeans = KMeans(n_clusters=10, random_state=42)  # Set number of clusters to 10
kmeans.fit(X_scaled)

# Get the cluster labels and centroids
cluster_labels = kmeans.labels_
centroids = kmeans.cluster_centers_

# Visualize the results using the first two features (or use PCA for dimensionality reduction)
plt.figure(figsize=(8, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=cluster_labels, cmap='viridis', s=5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=100, marker='X')  # Plot centroids
plt.title('K-Means Clustering of MNIST Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Cluster')
plt.show()

# Optional: evaluate the clustering performance using inertia (sum of squared distances)
print(f"Inertia: {kmeans.inertia_}")



""" Hierarchical Clustering

"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix

# Mount Google Drive and load the dataset
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

file_path = '/content/sample_data/mnist_train_small.csv'
df = pd.read_csv(file_path)

# Data Preprocessing
# Assuming the first column is the labels and the rest are pixel values
X = df.drop(columns=[df.columns[0]])  # Dropping the label column
y = df[df.columns[0]]  # Labels (if needed for validation)

# Normalize the data (important for clustering)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ---------------------- Gaussian Mixture Model (GMM) ----------------------

# Define and train the GMM model
gmm = GaussianMixture(n_components=10, random_state=42)  # 10 for 10 digits
gmm.fit(X_scaled)

# Predict the cluster for each data point
gmm_labels = gmm.predict(X_scaled)

# Plot a few examples and their corresponding GMM cluster
plt.figure(figsize=(10, 5))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(X.iloc[i].values.reshape(28, 28), cmap='gray')
    plt.title(f'Cluster: {gmm_labels[i]}')
    plt.axis('off')
plt.show()

# ------------------- Hierarchical Clustering -----------------------------

# Perform hierarchical clustering
hierarchical_clustering = AgglomerativeClustering(n_clusters=10)
hierarchical_labels = hierarchical_clustering.fit_predict(X_scaled)

# Plot dendrogram (subset of 100 samples for better visualization)
Z = linkage(X_scaled[:100], 'ward')
plt.figure(figsize=(10, 7))
dendrogram(Z)
plt.show()

# Plot a few examples and their corresponding Hierarchical cluster
plt.figure(figsize=(10, 5))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(X.iloc[i].values.reshape(28, 28), cmap='gray')
    plt.title(f'Cluster: {hierarchical_labels[i]}')
    plt.axis('off')
plt.show()

# ------------------- Evaluation (Optional) -------------------------------

# GMM Accuracy and Confusion Matrix (optional)
print("GMM Confusion Matrix:\n", confusion_matrix(y, gmm_labels))

# Hierarchical Clustering Accuracy and Confusion Matrix (optional)
print("Hierarchical Clustering Confusion Matrix:\n", confusion_matrix(y, hierarchical_labels))

"""Mathematical Modelling

"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Load the dataset
file_path = '/content/sample_data/mnist_train_small.csv'
df = pd.read_csv(file_path)

# Splitting the data into features (X) and labels (y)
X = df.drop('6', axis=1).values  # Assuming '6' is the label column
y = df['6'].values

# Normalize the features (scale pixel values to [0, 1])
X = X / 255.0

# One-hot encode the labels
lb = LabelBinarizer()
y = lb.fit_transform(y)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Building the neural network model
model = Sequential([
    Flatten(input_shape=(X_train.shape[1],)),  # Flatten input images
    Dense(128, activation='relu'),  # Hidden layer
    Dense(10, activation='softmax')  # Output layer for 10 classes (digits 0-9)
])

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',S
              metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)

print(f'Test accuracy: {test_acc}')

"""HMM Model Prediction"""

# Import necessary libraries
import pandas as pd
import numpy as np
from google.colab import drive
from hmmlearn import hmm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.exceptions import ConvergenceWarning
import warnings

# Ignore warnings related to convergence
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# Mount Google Drive to access data
drive.mount('/content/drive', force_remount=True)

# Load data
file_path = '/content/sample_data/mnist_train_small.csv'
df = pd.read_csv(file_path)

# Separate features (pixels) and labels (digits)
X = df.drop('6', axis=1).values  # Drop the first column ('6'), which contains the labels
y = df['6'].values  # The first column is the label

# Normalize the data (scale pixel values)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reduce dimensionality using PCA
pca = PCA(n_components=50)  # Reduce dimensions to 50 components
X_reduced = pca.fit_transform(X_scaled)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=42)

# Reshape data for HMM: Assume each image is a sequence of rows
# Reshape based on reduced PCA components, 50 in this case
sequence_length = 5  # Adjust based on new dimensionality, 50 PCA components can be reshaped as 5x10
X_train_reshaped = [X_train[i].reshape(sequence_length, -1) for i in range(X_train.shape[0])]
X_train_concat = np.concatenate(X_train_reshaped)

# Create correct lengths array
lengths = [sequence_length] * len(X_train_reshaped)

# Define and train an HMM model with adjusted parameters
model = hmm.GaussianHMM(n_components=3, covariance_type="diag", n_iter=1000, tol=0.01, random_state=42)

# Fit the model to the training data
model.fit(X_train_concat, lengths)

# Test the model on a test image sequence
test_image = X_test[0].reshape(sequence_length, -1)  # Reshape a test image into the correct sequence length
hidden_states = model.predict(test_image)

# Output the predicted hidden states for the test image
print("Predicted hidden states for the test image:", hidden_states)

# Calculate and print the log probability of the test image sequence
logprob = model.score(test_image)
print("Log probability of the test image sequence:", logprob)

"""Generalized Additive Models

"""

import pandas as pd
import numpy as np
from pygam import LogisticGAM, s
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the dataset
file_path = '/content/sample_data/mnist_train_small.csv'
df = pd.read_csv(file_path)

# Separate features (X) and target (y)
X = df.iloc[:, 1:]  # Features (pixel values)
y = df.iloc[:, 0]   # Target (digit labels)

# Convert multi-class labels to binary labels: Let's classify "3" vs. all other digits
y_binary = np.where(y == 3, 1, 0)  # 1 for digit "3", 0 for all other digits

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)

# Create a Generalized Additive Model for binary classification
gam = LogisticGAM(s(0) + s(1) + s(2) + s(3) + s(4))  # Using the first few features for simplicity

# Train the GAM model
gam.fit(X_train, y_train)

# Make predictions
y_pred = gam.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

# Visualize smooth functions for the first few features
import matplotlib.pyplot as plt
for i, term in enumerate(gam.terms):
    if term.isintercept:
        continue
    XX = gam.generate_X_grid(term=i)
    plt.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))
    plt.title(f'Smooth function for feature {i}')
    plt.show()